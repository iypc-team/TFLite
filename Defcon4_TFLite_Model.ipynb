{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlauq-4FWGZM"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division\n",
    "import itertools\n",
    "import os, shutil\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '5'\n",
    "from os.path import exists, join\n",
    "from time import sleep, perf_counter\n",
    "from BashColors import C\n",
    "from CV2_Utils_2 import *\n",
    "from TarfileFunctions import *\n",
    "\n",
    "contentPath = os.getcwd()\n",
    "cv2Path = join(contentPath, 'CV2Images')\n",
    "dataGeneratorPath = join(contentPath, 'DataGenerator')\n",
    "imagesPath = join(contentPath, 'images')\n",
    "\n",
    "if exists(dataGeneratorPath):\n",
    "    pass\n",
    "    # shutil.rmtree(dataGeneratorPath)\n",
    "else:\n",
    "    tff.extractTarfiles('DataGenerator5.tar.gz')\n",
    "sleep(1)\n",
    "\n",
    "if exists(imagesPath):\n",
    "    pass\n",
    "    # shutil.rmtree(imagesPath)\n",
    "else:\n",
    "    tff.extractTarfiles('images.tar.gz')\n",
    "sleep(1)\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "initialFilesGlob=glob.glob('**')\n",
    "\n",
    "print(f'TF version: {C.BIBlue}{tf.__version__}{C.ColorOff}')\n",
    "print(f'Hub version: {C.BIBlue}{hub.__version__}{C.BIRed}')\n",
    "print('GPU is', 'available' if tf.config.list_physical_devices('GPU') else 'NOT AVAILABLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanFiles(initial:list, delete=False):\n",
    "    currentFilesGlob=glob.glob('**')\n",
    "    # print(len(currentFilesGlob), len(initialFilesGlob))\n",
    "    for fil in currentFilesGlob:\n",
    "        if not fil in initial:\n",
    "            if isdir(fil):\n",
    "                print(f'{C.BIBlue}{fil}')\n",
    "                if delete:\n",
    "                    shutil.rmtree(fil)\n",
    "            elif isfile(fil):\n",
    "                print(f'{C.ColorOff}{fil}')\n",
    "CleanFiles(initialFilesGlob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelList=['efficientnetv2-s', 'efficientnetv2-m', 'efficientnetv2-l', 'efficientnetv2-s-21k', 'efficientnetv2-m-21k', 'efficientnetv2-l-21k', 'efficientnetv2-xl-21k', 'efficientnetv2-b0-21k', 'efficientnetv2-b1-21k', 'efficientnetv2-b2-21k', 'efficientnetv2-b3-21k', 'efficientnetv2-s-21k-ft1k', 'efficientnetv2-m-21k-ft1k', 'efficientnetv2-l-21k-ft1k', 'efficientnetv2-xl-21k-ft1k', 'efficientnetv2-b0-21k-ft1k', 'efficientnetv2-b1-21k-ft1k', 'efficientnetv2-b2-21k-ft1k', 'efficientnetv2-b3-21k-ft1k', 'efficientnetv2-b0', 'efficientnetv2-b1', 'efficientnetv2-b2', 'efficientnetv2-b3', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'bit_s-r50x1', 'inception_v3', 'inception_resnet_v2', 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152', 'nasnet_large', 'nasnet_mobile', 'pnasnet_large', 'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224']\n",
    "count=0\n",
    "additionsDict={}\n",
    "for model in modelList:\n",
    "    print(f'{count}. {model}')\n",
    "    if model.__contains__('mobilenet'):\n",
    "        newItem={model : 224}\n",
    "        additionsDict.update(newItem)\n",
    "        print(f'{count}. {C.BIGreen}{model}{C.ColorOff}')\n",
    "    count+=1\n",
    "    \n",
    "additionsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlsEcKVeuCnf"
   },
   "outputs": [],
   "source": [
    "# model_name = \"mobilenet_v2_100_224\" \n",
    "#model_name = 'mobilenet_v3_small_075_224'\n",
    "model_name = modelList[49]\n",
    "# @param ['efficientnetv2-s', 'efficientnetv2-m', 'efficientnetv2-l', 'efficientnetv2-s-21k', 'efficientnetv2-m-21k', 'efficientnetv2-l-21k', 'efficientnetv2-xl-21k', 'efficientnetv2-b0-21k', 'efficientnetv2-b1-21k', 'efficientnetv2-b2-21k', 'efficientnetv2-b3-21k', 'efficientnetv2-s-21k-ft1k', 'efficientnetv2-m-21k-ft1k', 'efficientnetv2-l-21k-ft1k', 'efficientnetv2-xl-21k-ft1k', 'efficientnetv2-b0-21k-ft1k', 'efficientnetv2-b1-21k-ft1k', 'efficientnetv2-b2-21k-ft1k', 'efficientnetv2-b3-21k-ft1k', 'efficientnetv2-b0', 'efficientnetv2-b1', 'efficientnetv2-b2', 'efficientnetv2-b3', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'bit_s-r50x1', 'inception_v3', 'inception_resnet_v2', 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152', 'nasnet_large', 'nasnet_mobile', 'pnasnet_large', 'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224']\n",
    "\n",
    "model_handle_map = {\n",
    "  \"efficientnetv2-s\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-s-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-xl-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2\",\n",
    "  \"efficientnetv2-b0-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/feature_vector/2\",\n",
    "  \"efficientnetv2-s-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-xl-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/feature_vector/2\",\n",
    "  \"efficientnetv2-b0-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/feature_vector/2\",\n",
    "  \"efficientnetv2-b0\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/feature_vector/2\",\n",
    "  \"efficientnet_b0\": \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\",\n",
    "  \"efficientnet_b1\": \"https://tfhub.dev/tensorflow/efficientnet/b1/feature-vector/1\",\n",
    "  \"efficientnet_b2\": \"https://tfhub.dev/tensorflow/efficientnet/b2/feature-vector/1\",\n",
    "  \"efficientnet_b3\": \"https://tfhub.dev/tensorflow/efficientnet/b3/feature-vector/1\",\n",
    "  \"efficientnet_b4\": \"https://tfhub.dev/tensorflow/efficientnet/b4/feature-vector/1\",\n",
    "  \"efficientnet_b5\": \"https://tfhub.dev/tensorflow/efficientnet/b5/feature-vector/1\",\n",
    "  \"efficientnet_b6\": \"https://tfhub.dev/tensorflow/efficientnet/b6/feature-vector/1\",\n",
    "  \"efficientnet_b7\": \"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\",\n",
    "  \"bit_s-r50x1\": \"https://tfhub.dev/google/bit/s-r50x1/1\",\n",
    "  \"inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/feature-vector/4\",\n",
    "  \"inception_resnet_v2\": \"https://tfhub.dev/google/imagenet/inception_resnet_v2/feature-vector/4\",\n",
    "  \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/feature-vector/4\",\n",
    "  \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/feature-vector/4\",\n",
    "  \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/feature-vector/4\",\n",
    "  \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4\",\n",
    "  \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/feature-vector/4\",\n",
    "  \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/feature-vector/4\",\n",
    "  \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/4\",\n",
    "  \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4\",\n",
    "  \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/4\",\n",
    "  \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\",\n",
    "  \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/feature_vector/4\",\n",
    "  \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4\",\n",
    "  \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5\",\n",
    "}\n",
    "\n",
    "model_image_size_map = {\n",
    "  \"efficientnetv2-s\": 384,\n",
    "  \"efficientnetv2-m\": 480,\n",
    "  \"efficientnetv2-l\": 480,\n",
    "  \"efficientnetv2-b0\": 224,\n",
    "  \"efficientnetv2-b1\": 240,\n",
    "  \"efficientnetv2-b2\": 260,\n",
    "  \"efficientnetv2-b3\": 300,\n",
    "  \"efficientnetv2-s-21k\": 384,\n",
    "  \"efficientnetv2-m-21k\": 480,\n",
    "  \"efficientnetv2-l-21k\": 480,\n",
    "  \"efficientnetv2-xl-21k\": 512,\n",
    "  \"efficientnetv2-b0-21k\": 224,\n",
    "  \"efficientnetv2-b1-21k\": 240,\n",
    "  \"efficientnetv2-b2-21k\": 260,\n",
    "  \"efficientnetv2-b3-21k\": 300,\n",
    "  \"efficientnetv2-s-21k-ft1k\": 384,\n",
    "  \"efficientnetv2-m-21k-ft1k\": 480,\n",
    "  \"efficientnetv2-l-21k-ft1k\": 480,\n",
    "  \"efficientnetv2-xl-21k-ft1k\": 512,\n",
    "  \"efficientnetv2-b0-21k-ft1k\": 224,\n",
    "  \"efficientnetv2-b1-21k-ft1k\": 240,\n",
    "  \"efficientnetv2-b2-21k-ft1k\": 260,\n",
    "  \"efficientnetv2-b3-21k-ft1k\": 300, \n",
    "  \"efficientnet_b0\": 224,\n",
    "  \"efficientnet_b1\": 240,\n",
    "  \"efficientnet_b2\": 260,\n",
    "  \"efficientnet_b3\": 300,\n",
    "  \"efficientnet_b4\": 380,\n",
    "  \"efficientnet_b5\": 456,\n",
    "  \"efficientnet_b6\": 528,\n",
    "  \"efficientnet_b7\": 600,\n",
    "  \"inception_v3\": 299,\n",
    "  \"inception_resnet_v2\": 299,\n",
    "  \"nasnet_large\": 331,\n",
    "  \"pnasnet_large\": 331,\n",
    "}\n",
    "model_image_size_map.update(additionsDict)\n",
    "model_handle = model_handle_map.get(model_name)\n",
    "pixels = model_image_size_map.get(model_name,  \n",
    "                                  model_image_size_map[model_name])\n",
    "\n",
    "print(f\"model_name: {model_name}\\nurl: {model_handle}\")\n",
    "\n",
    "IMAGE_SIZE = (pixels, pixels)\n",
    "print(f\"Input size {IMAGE_SIZE}\")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "print()\n",
    "# print(type(model_image_size_map))\n",
    "for key in model_image_size_map.items():\n",
    "    value = model_image_size_map.get(key)\n",
    "    # print(f'{C.ColorOff}{key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointPath=join(contentPath, 'Checkpoints_' + model_name)\n",
    "print(checkpointPath)\n",
    "\n",
    "earlyStop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', min_delta=0, patience=2, verbose=1,\n",
    "    mode='auto', baseline=None, restore_best_weights=True,\n",
    "    # print('\\n',\n",
    ")\n",
    "\n",
    "checkpoints = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpointPath,\n",
    "    monitor='loss', verbose=1, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', save_freq='epoch',\n",
    "    options=None\n",
    ")\n",
    "\n",
    "# checkpoint_dir = './trainingCheckpoints'\n",
    "checkpoint_dir = checkpointPath\n",
    "checkpoint_prefix = join(checkpoint_dir, 'epoch_{epoch}')\n",
    "print(checkpoint_prefix)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix, save_weights_only = False\n",
    ")\n",
    "print(model_image_size_map[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import RandomFlip, RandomTranslation\n",
    "\n",
    "def build_dataset(subset):\n",
    "    return tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        dataGeneratorPath,\n",
    "        shuffle = False,\n",
    "        validation_split = 0.2,\n",
    "        subset = subset,\n",
    "        label_mode = \"categorical\",\n",
    "        seed = 456, # Seed provided when using validation_split \n",
    "        # and shuffle  =  True.\n",
    "        # A fixed seed is used so that the validation set is stable \n",
    "        # across runs.\n",
    "        image_size = IMAGE_SIZE,\n",
    "        batch_size = BATCH_SIZE)\n",
    "\n",
    "train_ds  =  build_dataset(\"training\")\n",
    "class_names  =  tuple(train_ds.class_names)\n",
    "trainSteps  =  train_ds.cardinality().numpy()\n",
    "train_ds  =  train_ds.unbatch().batch(BATCH_SIZE)\n",
    "train_ds  =  train_ds.repeat()\n",
    "\n",
    "normalization_layer  =  tf.keras.layers.Rescaling(1. / 255)\n",
    "preprocessing_model  =  tf.keras.Sequential([normalization_layer])\n",
    "do_data_augmentation  =  True\n",
    "if do_data_augmentation:\n",
    "    preprocessing_model.add(tf.keras.layers.RandomRotation(40))\n",
    "    \n",
    "    preprocessing_model.add(RandomTranslation(0, 0.05))\n",
    "    preprocessing_model.add(RandomTranslation(0.05, 0))\n",
    "    preprocessing_model.add(RandomTranslation(0, 0.1))\n",
    "    preprocessing_model.add(RandomTranslation(0.1, 0))\n",
    "    preprocessing_model.add(RandomTranslation(0, 0.2))\n",
    "    preprocessing_model.add(RandomTranslation(0.2, 0))\n",
    "    # Like the old tf.keras.preprocessing.image.ImageDataGenerator(),\n",
    "    # image sizes fixed when reading, then a random zoom is applied.\n",
    "    # If training inputs larger than image_size, one could also use,\n",
    "    # RandomCrop with a batch size of 1 and rebatch later.\n",
    "    \n",
    "    preprocessing_model.add(tf.keras.layers.RandomZoom(0.2, 0.2))\n",
    "    preprocessing_model.add(tf.keras.layers.RandomZoom(-0.1, -0.1))\n",
    "    \n",
    "    preprocessing_model.add(RandomFlip(mode = \"horizontal\"))\n",
    "    preprocessing_model.add(RandomFlip(mode = \"vertical\"))\n",
    "    \n",
    "train_ds  =  train_ds.map(lambda images, labels:\n",
    "                        (preprocessing_model(images), labels))\n",
    "\n",
    "val_ds  =  build_dataset(\"validation\")\n",
    "validationSteps  =  val_ds.cardinality().numpy()\n",
    "val_ds  =  val_ds.unbatch().batch(BATCH_SIZE)\n",
    "val_ds  =  val_ds.map(lambda images, labels:\n",
    "                    (normalization_layer(images), labels))\n",
    "print(f'trainSteps: {trainSteps}\\tvalidationSteps: {validationSteps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=imagesPath,\n",
    "    labels=None,\n",
    "    batch_size=1,\n",
    "    image_size=(224, 224),\n",
    "    interpolation='bilinear',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RaJW3XrPyFiF"
   },
   "outputs": [],
   "source": [
    "do_fine_tuning = True \n",
    "\n",
    "print(f'model url:\\n{C.BIPurple}{model_handle}{C.ColorOff}')\n",
    "model = tf.keras.Sequential([\n",
    "    # Explicitly define the input shape\n",
    "    # so the model can be properly loaded by the TFLiteConverter\n",
    "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
    "    hub.KerasLayer(model_handle, trainable=do_fine_tuning),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(\n",
    "        len(class_names),\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "])\n",
    "model.build((None,)+IMAGE_SIZE+(3,))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9f3yBUvkd_VJ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "model.compile(\n",
    "    optimizer = SGD(learning_rate=0.1, momentum=0.9),\n",
    "    loss = CategoricalCrossentropy(from_logits=True,\n",
    "                                   label_smoothing=0.1),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_YKX2Qnfg6x"
   },
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = trainSteps\n",
    "VALIDATION_STEPS = validationSteps\n",
    "# STEPS_PER_EPOCH = 676 // BATCH_SIZE\n",
    "# VALIDATION_STEPS = 168 // BATCH_SIZE\n",
    "print(f'trainSteps: {trainSteps} validationSteps: {validationSteps}')\n",
    "print(STEPS_PER_EPOCH, VALIDATION_STEPS)\n",
    "\n",
    "start=perf_counter()\n",
    "hist = model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch = STEPS_PER_EPOCH,\n",
    "    validation_data = val_ds,\n",
    "    validation_steps = VALIDATION_STEPS,\n",
    "    epochs = 1,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False,\n",
    "    verbose = 1, # default='auto'\n",
    "    workers = 2,\n",
    "    use_multiprocessing = False,\n",
    "    callbacks = tf.keras.callbacks.History().model,\n",
    ")\n",
    "    # callbacks=[checkpoint_callback]).history\n",
    "\n",
    "finish=perf_counter()\n",
    "cvu.printTime(start, finish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.plot(hist.history['loss'], label = 'loss')\n",
    "plt.plot(hist.history['val_loss'], label='val loss')\n",
    "plt.title(\"Loss vs Val_Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = hist['accuracy']\n",
    "val_acc = hist['val_accuracy']\n",
    "\n",
    "loss = hist['loss']\n",
    "val_loss = hist['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "# plt.ylim([min(plt.ylim()),1])\n",
    "plt.ylim([min(plt.ylim()), 1.2])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([-4,4])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oi1iCNB9K1Ai"
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(val_ds))\n",
    "image = x[0, :, :, :]\n",
    "true_index = np.argmax(y[0])\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Expand the validation image to (1, 224, 224, 3)\n",
    "# before predicting the label\n",
    "prediction_scores = model.predict(np.expand_dims(image, axis=0))\n",
    "predicted_index = np.argmax(prediction_scores)\n",
    "print(\"True label: \" + class_names[true_index])\n",
    "print(\"Predicted label: \" + class_names[predicted_index])\n",
    "print(prediction_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "saved_model_path = f\"{contentPath}/Defcon4_{model_name}.h5\"\n",
    "print(saved_model_path)\n",
    "tf.saved_model.save(model, saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Va1Vo92fSyV6"
   },
   "outputs": [],
   "source": [
    "\n",
    "optimize_lite_model = True  # if True the model is trainable \n",
    "#Setting a value greater than zero enables quantization of neural network activations. A few dozen is already a useful amount.\n",
    "num_calibration_examples = 60  # min:0, max:1000, step:1\n",
    "representative_dataset = None\n",
    "if optimize_lite_model and num_calibration_examples:\n",
    "  # Use a bounded number of training examples without labels for calibration.\n",
    "  # TFLiteConverter expects a list of input tensors, each with batch size 1.\n",
    "    representative_dataset = lambda: itertools.islice(([image[None, ...]] for batch, _ in train_ds for image in batch),\n",
    "      num_calibration_examples)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
    "if optimize_lite_model:\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    if representative_dataset:  # This is optional, see above.\n",
    "        converter.representative_dataset = representative_dataset\n",
    "        \n",
    "lite_model_content = converter.convert()\n",
    "\n",
    "with open(f\"{contentPath}/Defcon4_{model_name}.tflite\", \"wb\") as f:\n",
    "    f.write(lite_model_content)\n",
    "print(\"Wrote %sTFLite model of %d bytes.\" %\n",
    "      (\"optimized \" if optimize_lite_model else \"\",\n",
    "       len(lite_model_content)))\n",
    "print(f'{len(lite_model_content) / 1024 / 1024} megabytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wqEmD0xIqeG"
   },
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=lite_model_content)\n",
    "# This little helper wraps the TFLite Interpreter as a numpy-to-numpy function.\n",
    "def lite_model(images):\n",
    "    interpreter.allocate_tensors()\n",
    "    interpreter.set_tensor(\n",
    "        interpreter.get_input_details()[0]['index'], images)\n",
    "    interpreter.invoke()\n",
    "    return interpreter.get_tensor(\n",
    "        interpreter.get_output_details()[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMMK-fZrKrk8"
   },
   "outputs": [],
   "source": [
    "#@markdown For rapid experimentation, start with a moderate number of examples.\n",
    "num_eval_examples = 25  #@param {type:\"slider\", min:0, max:700}\n",
    "eval_dataset = ((image, label)  # TFLite expects batch size 1.\n",
    "                for batch in train_ds\n",
    "                for (image, label) in zip(*batch))\n",
    "count = 0\n",
    "count_lite_tf_agree = 0\n",
    "count_lite_correct = 0\n",
    "for image, label in eval_dataset:\n",
    "    probs_lite = lite_model(image[None, ...])[0]\n",
    "    probs_tf = model(image[None, ...]).numpy()[0]\n",
    "    y_lite = np.argmax(probs_lite)\n",
    "    y_tf = np.argmax(probs_tf)\n",
    "    y_true = np.argmax(label)\n",
    "    count +=1\n",
    "    if y_lite == y_tf: count_lite_tf_agree += 1\n",
    "    if y_lite == y_true: count_lite_correct += 1\n",
    "    if count >= num_eval_examples: break\n",
    "print(\"TFLite model agrees with original model on %d of %d examples (%g%%).\" %\n",
    "      (count_lite_tf_agree, count, 100.0 * count_lite_tf_agree / count))\n",
    "print(\"TFLite model is accurate on %d of %d examples (%g%%).\" %\n",
    "      (count_lite_correct, count, 100.0 * count_lite_correct / count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "log=logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
    "print(log)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ScitaPqhKtuW"
   ],
   "name": "TF Hub for TF2: Retraining an image classifier",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
